{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    implementation inspired from https://datasciencelab.wordpress.com/2014/01/15/improved-seeding-for-clustering-with-k-means/\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.spatial.distance\n",
    "import sys\n",
    "\n",
    "\n",
    "class KPlusPlusGaussian:\n",
    "    def __init__(self, number_of_centers, x, stochastic=False, stochastic_n_samples=10000, random_seed=42):\n",
    "        \"\"\"\n",
    "        :param stochastic: When stochastic is True for faster calculation only keep a smaller subset \n",
    "                            of the data of size stochastic_n_samples\n",
    "        \"\"\"\n",
    "        assert len(x) >= number_of_centers\n",
    "        assert number_of_centers > 0\n",
    "\n",
    "        self.number_of_centers = number_of_centers\n",
    "        if stochastic and stochastic_n_samples < len(x):\n",
    "            idx = np.random.randint(len(x), size=stochastic_n_samples)\n",
    "            self.x = x[idx,:]\n",
    "        else:\n",
    "            self.x = x\n",
    "\n",
    "        self.overflow_avoid = len(x) + 1\n",
    "        self.centers = []\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "    def _distances(self, center):\n",
    "        # the maximum mass probability value is for the center itself\n",
    "        # this is definitely an integer as the centers are chosen from the dataaset\n",
    "        return np.array([1/distance.euclidean(x, center) for x in self.x])\n",
    "\n",
    "    def _dist_from_centers_initialize(self):\n",
    "        testing_center = self.centers[len(self.centers) - 1]\n",
    "        self.distances = self._distances(testing_center)\n",
    "        \n",
    "    def _dist_from_centers(self):\n",
    "        testing_center = self.centers[len(self.centers) - 1]\n",
    "        self.distances = np.min(np.column_stack((self._distances(testing_center), self.distances.T)), axis=1)\n",
    "\n",
    "    def _choose_next_center(self):\n",
    "        # avoid overflow\n",
    "        self.distances[self.distances > np.finfo(np.float64).max / self.overflow_avoid] =  np.finfo(np.float64).max / self.overflow_avoid\n",
    "        \n",
    "        self.probabilities = self.distances / self.distances.sum()\n",
    "        self.cumulativeProbabilities = self.probabilities.cumsum()\n",
    "        r = random.random()\n",
    "        ind = np.where(self.cumulativeProbabilities >= r)[0][0]\n",
    "        return self.x[ind]\n",
    "\n",
    "    def init_centers(self, verbose=0):\n",
    "        random.seed(self.random_seed)\n",
    "        \n",
    "        center = random.randint(0, len(self.x))\n",
    "        self.centers.append(self.x[center])\n",
    "        if verbose > 0:\n",
    "            print('Centers found:', len(self.centers))\n",
    "        self._dist_from_centers_initialize()\n",
    "        while len(self.centers) < self.number_of_centers:\n",
    "            self.centers.append(self._choose_next_center())\n",
    "            if verbose > 0:\n",
    "                print('Centers found:', len(self.centers))\n",
    "            if len(self.centers) < self.number_of_centers:\n",
    "                self._dist_from_centers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "from dsio.anomaly_detectors import AnomalyMixin\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy.stats.distributions\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def gaussian(x, l, s):\n",
    "    return_value = 1\n",
    "    for x_i, l_i, s_i in zip(x, l, s):\n",
    "        return_value *= scipy.stats.norm(l_i, s_i).pdf(x_i)\n",
    "    return return_value\n",
    "\n",
    "\n",
    "class OnlineEMGaussian(AnomalyMixin):\n",
    "    def __init__(self, gammas, lambdas, segment_length, sigmas=None, update_power=1.0, verbose=0):\n",
    "        \"\"\"\n",
    "        :param gammas: \n",
    "        :param lambdas: \n",
    "        :param segment_length: \n",
    "        :param n_clusters: the different profiles to create for the kind of users \n",
    "        :param update_power: the power that determmines the update faktor in each iteration of the online algorithm\n",
    "        \"\"\"\n",
    "        # gammas and lambdas are the initialization\n",
    "        self.gammas = np.array(gammas)\n",
    "        self.lambdas = np.vstack(lambdas)\n",
    "        self.segment_length = segment_length\n",
    "        \n",
    "        assert self.lambdas.ndim > 1\n",
    "        \n",
    "        if sigmas is not None:\n",
    "            self.sigmas = sigmas\n",
    "        else:\n",
    "            self.sigmas = np.vstack([[10 for _ in range(len(lambdas[0]))]  for _ in range(len(lambdas))])\n",
    "\n",
    "        assert len(gammas) == len(lambdas)\n",
    "        assert len(gammas) == len(sigmas)\n",
    "\n",
    "        # number of poisson mixtures\n",
    "        self.m = len(gammas)\n",
    "        # the dimension of the Poisson distribution\n",
    "        self.dim = len(self.lambdas[0])\n",
    "\n",
    "        # number of current iteration\n",
    "        self.iteration_k = 1\n",
    "\n",
    "        self.update_power = update_power\n",
    "\n",
    "        # a dictionary containing for each host valuable information\n",
    "        self.hosts = {}\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # HMM matrix\n",
    "        self.hard_transition_matrix = np.eye(self.m)\n",
    "        self.soft_transition_matrix = np.eye(self.m)\n",
    "\n",
    "    def calculate_participation(self, data):\n",
    "        \"\"\"\n",
    "        :param data: n array of the data to train\n",
    "        :return: an (n, m) array of the participation of each data point to each poisson distribution\n",
    "                m is the number of distributions\n",
    "        \"\"\"\n",
    "        f = np.zeros(shape=(len(data), self.m))\n",
    "        for i, x in enumerate(data):\n",
    "            participation = self.gammas * np.array([gaussian(x, lambda_i, sigma_i) for lambda_i, sigma_i in zip(self.lambdas, self.sigmas)])\n",
    "            total_x = np.sum(participation)\n",
    "            \n",
    "            # TODO\n",
    "            if total_x == 0:\n",
    "                participation = np.array([1/self.m] * self.m)\n",
    "                total_x = 1\n",
    "            f[i] = participation / total_x\n",
    "        return f\n",
    "\n",
    "    # TODO take into account the size of the batch\n",
    "    def calculate_likelihood(self, data):\n",
    "        # naive implementation for likelihood calculation\n",
    "        new_likelihood = 0\n",
    "        for x in data:\n",
    "            total_x = np.sum(self.gammas * np.array([gaussian(x, lambda_i, sigma_i) for lambda_i, sigma_i in zip(self.lambdas, self.sigmas)]))\n",
    "            new_likelihood = new_likelihood + log(total_x)\n",
    "        return new_likelihood\n",
    "\n",
    "    def update_parameters(self, batch):\n",
    "        \"\"\"\n",
    "        :param data: the batch data \n",
    "        updates gammas, lambdas and likelihood\n",
    "        \"\"\"\n",
    "\n",
    "        data = batch[:, :-1]\n",
    "\n",
    "        self.iteration_k += 1\n",
    "        n = len(data)\n",
    "        if n <= 0:\n",
    "            return\n",
    "        assert len(data[0]) == len(self.lambdas[0])\n",
    "\n",
    "        f = self.calculate_participation(data)\n",
    "\n",
    "        # update gammas and lambdas\n",
    "        temp_sum = f.sum(axis=0)\n",
    "\n",
    "        update_factor = 1 / (pow(self.iteration_k, self.update_power))\n",
    "\n",
    "        self.gammas = (1 - update_factor) * self.gammas + update_factor * (temp_sum / n)\n",
    "\n",
    "        # update lambdas\n",
    "        temp = np.zeros(shape=(self.m, self.dim))\n",
    "        for i, x in enumerate(data):\n",
    "            temp = temp + np.vstack([x * f_i for f_i in f[i]])\n",
    "        new_lambdas = np.vstack([temp[i] / temp_i for i, temp_i in enumerate(temp_sum)])\n",
    "\n",
    "        self.lambdas = (1 - update_factor) * self.lambdas + update_factor * new_lambdas\n",
    "        \n",
    "        # update sigmas\n",
    "        temp = np.zeros(shape=(self.m, self.dim))\n",
    "        for i, x in enumerate(data):\n",
    "            temp = temp + np.vstack([np.power(x - l_i, 2) * f_i for f_i, l_i in zip(f[i], self.lambdas)])\n",
    "        new_sigmas = np.vstack([temp[i] / temp_i for i, temp_i in enumerate(temp_sum)])\n",
    "\n",
    "        self.sigmas = (1 - update_factor) * self.sigmas + update_factor * new_sigmas\n",
    "        \n",
    "        # upon initialization self.hosts should not contain a key for host\n",
    "        # TODO memory intensive\n",
    "        for point in batch:\n",
    "            self.update_host(point)\n",
    "\n",
    "    def get_new_batch(self, data, pos):\n",
    "        n = len(data)\n",
    "\n",
    "        assert self.segment_length <= n\n",
    "\n",
    "        if self.segment_length + pos <= n:\n",
    "            return data[pos: pos + self.segment_length], pos + self.segment_length\n",
    "\n",
    "        return data[pos:], n\n",
    "\n",
    "    def closest_centers(self, data):\n",
    "        n = len(data)\n",
    "\n",
    "        f = self.calculate_participation(data)\n",
    "\n",
    "        # update gammas and lambdas\n",
    "        temp_sum = f.sum(axis=0)\n",
    "        return temp_sum / n\n",
    "\n",
    "    def update_host(self, point):\n",
    "        host = point[-1]\n",
    "        if host in self.hosts:\n",
    "            host_points = self.hosts[host]['n_points']\n",
    "\n",
    "            point_center = self.closest_centers([point])\n",
    "            # point_center = np.array([-pow(x-0.5, 2) if x < 0.5 else pow(x-0.5, 2) for x in point_center]) * 2 + 0.5\n",
    "\n",
    "            self.hosts[host]['group'] = (point_center + self.hosts[host]['group'] * host_points) / \\\n",
    "                                        (host_points + 1)\n",
    "\n",
    "            # the number of data points for the host\n",
    "            self.hosts[host]['n_points'] += 1\n",
    "\n",
    "            ###\n",
    "            # update transpose matrix\n",
    "            previous_point = self.hosts[host]['hard_previous']\n",
    "\n",
    "            closest_center = np.argmax(point_center)\n",
    "\n",
    "            new_transpose = np.zeros(self.m)\n",
    "            new_transpose[closest_center] = 1\n",
    "\n",
    "            points_for_cluster = self.hard_points_per_EM_cluster[previous_point]\n",
    "\n",
    "            self.hard_transition_matrix[previous_point] = (self.hard_transition_matrix[previous_point] *\n",
    "                                                           points_for_cluster + new_transpose) / \\\n",
    "                                                          (points_for_cluster + 1)\n",
    "\n",
    "            for i, previous in enumerate(self.hosts[host]['soft_previous']):\n",
    "                self.soft_transition_matrix[i] = (self.soft_transition_matrix[i] * self.soft_points_per_EM_cluster[i] +\n",
    "                                                  point_center * previous) / (self.soft_points_per_EM_cluster[i] +\n",
    "                                                                              previous)\n",
    "                self.soft_points_per_EM_cluster[i] += previous\n",
    "                \n",
    "                self.hosts[host]['soft_transition_matrix'] = (self.hosts[host]['soft_transition_matrix'] * self.hosts[host]['soft_points_per_cluster'][i] + point_center * previous) / (self.hosts[host]['soft_points_per_cluster'][i] +\n",
    "                                                                              previous)\n",
    "                self.hosts[host]['soft_points_per_cluster'][i] += previous\n",
    "                \n",
    "\n",
    "            self.hosts[host]['hard_previous'] = closest_center\n",
    "            self.hosts[host]['soft_previous'] = point_center\n",
    "            self.hard_points_per_EM_cluster[previous_point] += 1\n",
    "            \n",
    "            \n",
    "            points_for_cluster_host = self.hosts[host]['points_per_cluster'][previous_point]\n",
    "            self.hosts[host]['transition_matrix'][previous_point] = (self.hosts[host]['transition_matrix'][previous_point] *\n",
    "                                                           points_for_cluster_host + new_transpose) / \\\n",
    "                                                          (points_for_cluster_host + 1)\n",
    "            self.hosts[host]['points_per_cluster'][previous_point] += 1\n",
    "\n",
    "        else:\n",
    "            self.hosts[host] = {}\n",
    "            # create a self.m array containing the proportion of participation for this host for every center of poisson\n",
    "\n",
    "            point_center = self.closest_centers([point])\n",
    "            self.hosts[host]['group'] = point_center\n",
    "\n",
    "            closest_center = np.argmax(point_center)\n",
    "            self.hosts[host]['hard_previous'] = closest_center\n",
    "            self.hosts[host]['soft_previous'] = point_center\n",
    "            # self.hosts[host]['group'] = np.array(\n",
    "            #    [-pow(x - 0.5, 2) if x < 0.5 else pow(x - 0.5, 2) for x in point_center]) * 2 + 0.5\n",
    "\n",
    "            # the number of data points for the host\n",
    "            self.hosts[host]['n_points'] = 1\n",
    "            \n",
    "            # Host specific HMM\n",
    "            self.hosts[host]['transition_matrix'] = np.eye(self.m)\n",
    "            self.hosts[host]['points_per_cluster'] = np.zeros(self.m)\n",
    "            self.hosts[host]['soft_transition_matrix'] = np.eye(self.m)\n",
    "            self.hosts[host]['soft_points_per_cluster'] = np.zeros(self.m)\n",
    "\n",
    "    def fit(self, x):\n",
    "        \"\"\"\n",
    "        For fitting the initial values update function is called the pth column holds the by attribute\n",
    "        x is a array n times p where \n",
    "        :param x: data\n",
    "        \"\"\"\n",
    "        if len(x) <= 0:\n",
    "            return\n",
    "\n",
    "        features = len(x[0])\n",
    "        # the starting position of the current batch in the data\n",
    "\n",
    "        pos = 0\n",
    "        while pos < len(x):\n",
    "            batch, pos = self.get_new_batch(x, pos)\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                print('Running for data till position', pos, 'from total', len(x))\n",
    "\n",
    "            self.update_parameters(batch)\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print('Running clustering algorithm')\n",
    "\n",
    "        closest_centers = []\n",
    "\n",
    "        for host in self.hosts.keys():\n",
    "            closest_centers.append(self.hosts[host]['group'])\n",
    "\n",
    "        self.kMeans.fit(closest_centers)\n",
    "\n",
    "        for host in self.hosts.keys():\n",
    "            category = self.kMeans.predict([self.hosts[host]['group']])[0]\n",
    "            self.hosts[host]['category'] = category\n",
    "            points_in_cluster = self.hosts_per_kMeans_cluster[category]\n",
    "\n",
    "            self.probabilities_per_kMean_cluster[category] = \\\n",
    "                (self.probabilities_per_kMean_cluster[category] * points_in_cluster + self.hosts[host]['group']) / \\\n",
    "                (points_in_cluster + 1)\n",
    "\n",
    "            self.hosts_per_kMeans_cluster[category] += 1\n",
    "\n",
    "    def update(self, x):\n",
    "        \"\"\"\n",
    "        :param data: dictionary?\n",
    "        \"\"\"\n",
    "        # TODO (or another way to get the host name)\n",
    "        #                assumes the data has the appropriate length fot batch processing\n",
    "\n",
    "        if len(x) <= 0:\n",
    "            return\n",
    "\n",
    "        features = len(x[0])\n",
    "        data = x[:, 0:features - 1]\n",
    "        self.update_parameters(data)\n",
    "        for point in x:\n",
    "            self.update_host(point)\n",
    "\n",
    "            # kMeans center should be updated every a number of batch updates??\n",
    "\n",
    "    def score_anomaly_for_category(self, x, category=None, host=None):\n",
    "        pass\n",
    "\n",
    "    # TODO\n",
    "    def score_anomaly(self, x):\n",
    "        pass\n",
    "\n",
    "    # TODO\n",
    "    def flag_anomaly(self, x):\n",
    "        pass\n",
    "\n",
    "    def get_bic(self, data):\n",
    "        \"\"\"\n",
    "        :return a tuple of the bic avg_log_likelihoods and the log likelihood of the whole data\n",
    "        \"\"\"\n",
    "        return ((-2) / self.iteration_k) * self.calculate_likelihood(data) + log(len(data)) * (\n",
    "            2 * self.m - 1), self.calculate_likelihood(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats.distributions\n",
    "\n",
    "scipy.stats.norm(100, 10).cdf(90)\n",
    "\n",
    "a = np.array([1,2])\n",
    "b = np.array([1,4])\n",
    "\n",
    "np.power(a - b, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
